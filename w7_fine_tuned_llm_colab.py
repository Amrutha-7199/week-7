# -*- coding: utf-8 -*-
"""W7-Fine_Tuned_LLM_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11RlHzlWrhZcOcLAuBpd9YwBeptKufASU
"""

# Fine-Tuned LLM for Domain-Specific Tasks - Google Colab Notebook

# Install required libraries
!pip install transformers datasets torch

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
import pandas as pd

# Load a tokenizer and model for fine-tuning
model_name = "gpt2"  # Change to "google/t5-small" or "tiiuae/falcon-7b" as needed
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Load a dataset for fine-tuning (Medical Q&A example)
dataset = load_dataset("csv", data_files="CancerQA.csv")


# Split Dataset into 80% Training and 20% Validation
dataset = dataset["train"].train_test_split(test_size=0.2, seed=42)
tokenizer.pad_token = tokenizer.eos_token
# Tokenization function
#def tokenize_function(examples):
#    return tokenizer(examples["question"], padding="max_length", truncation=True, max_length=128)

def tokenize_function(examples):
    inputs = tokenizer(examples["question"], padding="max_length", truncation=True)
    targets = tokenizer(examples["answer"], padding="max_length", truncation=True)

    # Set labels equal to input_ids (Shifted for causal language modeling)
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",        # Now evaluation will work
    num_train_epochs=3,      # Number of training epochs
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    save_steps=50,
    report_to="wandb",
    save_total_limit=2,
    disable_tqdm=False,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],   # âœ… Pass Validation Dataset
)

# Fine-tune the model
#trainer.train()

# Save the Fine-Tuned Model
#trainer.save_model("./CancerQA_Model")


# Save the Fine-Tuned Model and Tokenizer
#model.save_pretrained("./CancerQA_Model")  # Save model locally
#tokenizer.save_pretrained("./CancerQA_Model")  # Save tokenizer locally

print("Starting training...")
trainer.train()
wandb.finish()  # Ensures the sync is complete
print("Training completed.")



# Save the Fine-Tuned Model
#trainer.save_model("./CancerQA_Model")


# Save the Fine-Tuned Model and Tokenizer
#model.save_pretrained("./CancerQA_Model")  # Save model locally
#tokenizer.save_pretrained("./CancerQA_Model")  # Save tokenizer locally

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the Fine-Tuned Model (remove ./ from the path)
model = AutoModelForSeq2SeqLM.from_pretrained("CancerQA_Model")  # Without './'
tokenizer = AutoTokenizer.from_pretrained("t5-small")  # Your tokenizer name

# Example inference function
def generate_answer(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    output = model.generate(input_ids, max_length=100, num_return_sequences=1)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Test the Fine-Tuned Model
query = "What are the symptoms of cancer?"
answer = generate_answer(query)
print("Q:", query)
print("A:", answer)

pip install streamlit transformers torch

import streamlit as st
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained("/content/CancerQA_Model")  # Path to your fine-tuned model
tokenizer = AutoTokenizer.from_pretrained("/content/CancerQA_Model")  # Path to your fine-tuned tokenizer

# Define the function to generate answers
def generate_answer(query):
    # Tokenize the input query
    input_ids = tokenizer(query, return_tensors="pt").input_ids
    # Generate the answer using the model
    output = model.generate(input_ids, max_length=100, num_return_sequences=1)
    # Decode the generated output to get the response
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Streamlit app layout
st.title("Cancer Question Answering System")
st.write("Ask me any question related to cancer, and I'll provide an answer based on my training.")

# Input field for user to enter a query
query = st.text_input("Ask your question:")

# If the user has entered a query, generate the answer
if query:
    answer = generate_answer(query)
    st.write(f"**Answer:** {answer}")

streamlit run app.py